#!/bin/bash
#SBATCH -A CSC662                  # ← YOUR PROJECT ACCOUNT
#SBATCH -J ddp_tutorial             # ← Job name
#SBATCH --nodes=1                   # 1 Frontier node = 8 GPUs
#SBATCH --gres=gpu:8
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=7
#SBATCH -t 00:30:00
#SBATCH -q debug
#SBATCH -o ddp-%j.out
#SBATCH -e ddp-%j.err

# ----- REQUIRED FOR FRONTIER -----

module load PrgEnv-gnu
module load rocm/6.3.1
module load craype-accel-amd-gfx90a

module unload darshan-runtime
module unload libfabric
module load libfabric/1.22.0

# ----- PYTHON ENVIRONMENT -----
source ~/miniconda3/etc/profile.d/conda.sh
conda activate /lustre/orion/csc662/world-shared/topcicekd/pytorch_env


echo "Using python: $(which python)"

# ----- RECOMMENDED FRONTIER RCCL/NCCL FLAGS -----
export FI_MR_CACHE_MONITOR=kdreg2
export FI_CXI_DEFAULT_CQ_SIZE=131072
export FI_CXI_DEFAULT_TX_SIZE=2048
export FI_CXI_RX_MATCH_MODE=hybrid

export NCCL_NET_GDR_LEVEL=3
export NCCL_CROSS_NIC=1
export NCCL_SOCKET_IFNAME=hsn0
export TORCH_NCCL_HIGH_PRIORITY=1

export MIOPEN_DISABLE_CACHE=1
export NCCL_PROTO=Simple
export OMP_NUM_THREADS=7

# Add these 3 lines from reference:
export MIOPEN_USER_DB_PATH=/tmp/$SLURM_JOB_ID
mkdir -p $MIOPEN_USER_DB_PATH
export LD_PRELOAD=/lib64/libgcc_s.so.1:/usr/lib64/libstdc++.so.6


export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)
export MASTER_PORT=29500
export WORLD_SIZE=$SLURM_NTASKS
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID

echo "MASTER_ADDR=$MASTER_ADDR, MASTER_PORT=$MASTER_PORT"

# ----- RUN YOUR PYTORCH DDP SCRIPT -----

echo "Starting DDP training..."

time srun -n 8 python ddp_main.py 10 1 --batch_size 32
